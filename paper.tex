\documentclass{IEEEtran}

\usepackage{graphicx}
\usepackage{color}
\usepackage[bookmarks=false]{hyperref}

\newcommand{\todo}[1]{\marginpar{\parbox{18mm}{\flushleft\tiny\color{red}\textbf{TODO}:
      #1}}}

\begin{document}

\title{Performance Evaluation of Big Data Processing Strategies for Neuroimaging}

\author{
  \IEEEauthorblockN{
    Val\'erie Hayot-Sasson and 
    Tristan Glatard
  }\\
  \IEEEauthorblockA{
    Department of Computer Science and Software Engineering, Concordia University, Montreal, Qu\'ebec, Canada
  }
}

\maketitle

\begin{abstract}
  
\end{abstract}

% Talk about containers somewhere?

\section{Introduction} % 1 page with abstract

Big Data processing engines leverage data locality, 
in-memory computing, and lazy evaluation to reduce the impact of data 
manipulation on the performance of applications. From early MapReduce 
implementations~\cite{dean2008mapreduce} to Apache Spark~\cite{zaharia2016apache} and now 
Dask~\cite{rocklin2015dask}, these frameworks have become 
mainstream tools for data analytics. Meanwhile, several scientific 
domains including bioinformatics, physics or astronomy, have entered 
the Big Data era due to increasing data volumes and variety. 
However, the adoption of Big Data engines for scientific data analysis 
remains limited, perhaps due to the widespread availability of 
scientific processing engines such as Pegasus~\cite{deelman2005pegasus}, 
Taverna~\cite{oinn2004taverna}, and many others. 

Scientific applications slightly differ from the typical Big Data use 
cases, which might explain the remaining gap between Big Data and 
scientific engines. While Big Data applications mostly target text 
processing (Web search, frequent pattern mining, recommender systems, 
etc~\cite{leskovec2014mining}), implemented in consistent software libraries, and 
running on clouds or dedicated commodity clusters with locality-aware 
file systems such as HDFS~\cite{shvachko2010hadoop}, scientific applications often 
involve binary data such as images and signals, processed by 
command-line tools using a mix of programming languages (C, Fortran, 
Python, shell scripts) and deployed on large, shared clusters where 
data is fetched from data nodes to compute nodes using shared file 
systems such as Lustre~\cite{schwan2003lustre}. Such differences in applications 
and infrastructures have important consequences. To mention only one, 
in-memory computing requires instrumentation to be applied to 
command-line tools.

Technological advances of the past decade, in particular page caching 
in the Linux kernel~\cite{love2010linux}, in-memory file systems 
(\texttt{tmpfs}) and memory-mapped files\todo{check that} might also 
explain the lack of adoption of Big Data engines for scientific 
applications. 

In-memory computing would then be a feature provided by 
the operating system rather than by the engine itself. The frontier 
between these two components is blurred and needs to be clarified.


% https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20130001600.pdf

Neuroimaging, our primary field of interest, is no exception to the 
generalized rise of data volumes in science, due to the joint increase 
of image resolution and subject cohort sizes~\cite{van2014human}. 
Processing engines have been developed with neuroinformatics 
applications in mind, for instance Nipype~\cite{gorgolewski2011nipype} 
or PSOM~\cite{bellec2012pipeline}. Big Data engines have been used for 
neuroimaging applications too, including in the Thunder 
project~\cite{freeman2014mapping} and in more specific works such 
as~\cite{makkie2019fast}. However, no quantitative performance 
evaluation has been conducted on neuroimaging applications to assess the 
added-value of Big Data engines compared to traditional processing engines.

This paper addresses the following questions:
\begin{enumerate}
\item What is the effect of in-memory computing and data locality on neuroimaging applications?
\item Can in-memory computing be effectively enabled by the operating system rather than the data processing engine?
\end{enumerate}

Answers to these questions have important implications. 
In~\cite{mehta2017comparative}, a comparative study of Dask, Spark, 
TensorFlow, MyriaDB, and SciDB on neuroinformatics use-cases is 
presented. It  concludes that these systems need to be extended to 
better address scientific code integration, data partitioning, data 
formats and system tuning. The work in~\cite{samba} also extended Spark 
with a better provenance system for scientific applications. We argue 
that such efforts should only be conducted if substantial performance 
improvements are expected from in-memory computing, lazy 
evaluation\todo{are we talking about lazy evaluation at all?} or data 
locality. On the other hand, scientific data processing engines are 
still being developed, and it is legitimate to wonder whether these 
projects should just be migrated to Spark, Dask, or other Big Data 
engines.

Our study focuses on performance. We intentionally do not intend to 
compare Big Data and scientific data processing engines on the grounds 
of workflow language expressivity, provenance capture and 
representation, portability or reproducibility, which are otherwise 
critical concerns. Besides, our study of performance focuses on the 
impact of data writes and transfers. It purposely leaves out task 
scheduling to computing resources, to focus on the understanding of 
data writes and movement. Task scheduling will be part of our 
discussion though.

\todo{Perhaps talk about modeling and simulation here.}

Our methods, including performance models, processing engines, \todo{Make this paragraph more informative}
applications and infrastructure used are described in 
Section~\ref{sec:methods}. Section~\ref{sec:results} presents our 
results which we discuss in Section~\ref{sec:discussion}. 
Section~\ref{sec:conclusion} concludes on the answer to our two research questions.



\section{Materials and Methods} % 4 pages
\label{sec:methods}

\subsection{Engines} % 1.5 pages

\subsubsection{Apache Spark}

%% Main concepts
% pipeline description
% detailed description relevant to the experiment
% how shuffle works
% how data locality is implemented
% spills to disk when dataset (and derived data) is too large for memory
% Lazy evaluation
% persistence (refer to Thunder)
% multiple schedulers: we focus on Spark Standalone
% overlay cluster: not considered here as this would be scheduling

%% Technical details
% data serialization to Java (we use Python as most neuroinformatics do)
% limitation in size of RDD element, due to java implementation of binary data
% memory management: don't give too much memory to executors or GC
%  many places recommend limiting to 5 cores/executor, no one explains why
% more on tuning?

\subsubsection{Nipype}

% pipeline description: map nodes and their limitations
% general description relevant to the experiment
% two plugins
% provenance
% file system caching
% memory management (bad?)

\subsection{Performance Models}

\subsubsection{In-Memory File System}

\subsubsection{Local Disk (Page Caching)} % 0.5 page

% Perhaps refer to this: https://ieeexplore.ieee.org/abstract/document/5496998

% 15 cores per executor can lead to bad I/O throughput
% http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/

% measured disk bandwidth with random I/O, very different from sequential

The Linux kernel uses in-memory page caching to speed-up disk writes. 
When a process calls the \texttt{write()} system call, data is copied 
to a memory cache that is asynchronously written to disk by flusher 
threads when memory shrinks, ``dirty" (unwritten) data grows, or a 
process invokes the \texttt{sync()} system call~\cite{love2010linux}. 
The flushing of page cache to disk is also called the writeback 
process.

Page caching has a dramatic effect on I/O, reducing the duration of 
disk writes by several orders of magnitude. It is essentially a way to 
emulate in-memory computing at the kernel level, without requiring a 
dedicated engine. The size of the page cache, however, becomes a 
limitation when processes write faster than the disk bandwidth. When 
this happens, the page cache rapidly fills up and writes are limited by 
the disk write bandwidth -- typically 500~MB/s for random writes to 
contemporary SSDs  -- as if no page cache was involved.

We introduce the following basic model to describe the filling and 
flushing of the page cache by an application:
$$
d(t) = \left( \frac{D}{C} - \frac{\delta}{\gamma} \right)t + d_0,
$$
where:
\begin{itemize}
\item $d(t)$ is the amount of data in the page cache at time t
\item $D$ is the total amount of data written by the application
\item $C$ is the total CPU time of the application
\item $\delta$ is the disk bandwidth
\item $\gamma$ is the max number of concurrent processes
\item $d_0$ is the amount of data in the page cache at time $t_0$
\end{itemize}

This model applies to parallel applications assuming that (1) 
concurrent processes all write the same amount of data, (2) 
concurrent processes all consume the same CPU time, (3) data is written 
uniformly along task execution. Due these assumptions, all the 
processes will write at the same rate, which explains why the model 
does not depend on the total number of concurrent processes in the 
application, but only on the max number of concurrent processes 
executing on the same node ($\gamma$). We realize that these 
assumptions would usually be violated in practice, but this simple 
model already provides interesting insights on the performance of disk 
writes, as shown later. Naturally, the model also ignores other 
processes that might be writing to disk concurrently to the 
application, which we assume negligible here. 

In general, an application should ensure that $\dot d$ remains negative 
or null, leading to the following inequality:
\begin{equation}
\frac{D}{C} \leq \frac{\delta}{\gamma} \label{eq:page-cache-inequality}
\end{equation}
This defines a data-compute ratio beyond which the page cache becomes 
asymptotically useless, which we will use to define our benchmarks. It 
should be noted that leveraging the page cache, and therefore ensuring 
that Equation~\ref{eq:page-cache-inequality} holds, has important 
performance implications: with page caching, the application will 
experience an almost negligible write time, while without page caching 
the write throughput will be that of the disk.

% Cache eviction: LRU/n.
% page caching is mentioned in https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20130001600.pdf too
% Note: writeback is dangerous as a system failure could result in data loss.

\subsubsection{Shared File System}

% is there a page cahe when writing to a socket? there must be!
% anyway the model is in the form of Eq 1, but now delta is the network bandwidth
% and gamma is the total number of concurrent processes.
% also, is lustre going to slow down network writes?

%~ In any case, I think we want the following inequality to hold:
%~ ```
%~ D/C <= \Delta / \Gamma
%~ ```
%~ where Delta is now the apparent bandwidth of the file system and Gamma 
%~ is the max number of concurrent tasks in the cluster. Whether page 
%~ caching is used or not will affect what happens when the inequality 
%~ holds: if page caching is here, write time will be 0, otherwise it will 
%~ be non-zero. If the inequality doesn't hold then we have congestion and 
%~ it's a different game: transfer times might tend to infinity and data 
%~ loss is to be expected.


% Say that we don't use HDFS because we focus on clusters and HDFS on multi-tenant clusters is difficult (refer to hadoop on demand).


% Make a table to explain where data locality and in-memory computing are
% available.  As a summary of the models.

% What happens when dataset and derived data is too large for disk?

In~\cite{saini2012performance} the Lustre file system is benchmarked on NASA applications. 

\subsection{Simulation} % is it included? 0.5 page

% Even if we end up not using simulation, here we should explain
% why the model in simgrid is limited. Refer to Fred's paper at CCGrid 2017.
% The following sub-sections become a set of models that we could implement
% in simulation to make it more realistic.

% Talk about ~\cite{lebre2015adding} and \cite{wrench}. Most simulation toolkits have focused on task 
%  scheduling. And https://dl.acm.org/citation.cfm?id=3041715


\subsection{Task Scheduling}

% Explain how we ensured that both engines lead to the same scheduling.
% Talk about pilot jobs and say we don't consider them here.

\subsection{Experiments} % 1.5 pages

\subsubsection{Infrastructure} % 1/4 a page (half a col)

% Dedicated Compute nodes with SLURM
% Lustre server

\begin{table}
\centering
\begin{tabular}{c|c}
$\delta/\gamma$ & $\Delta/\Gamma$\\
\hline
50~MB/s         & 5~MB/s
\end{tabular}
\label{table:infrastructure}
\caption{Bandwidth thresholds on the target cluster}
\end{table}


\subsubsection{Datasets} % 1/4 of a page (half a col)

% BigBrain
% Some BIDS dataset for fmriprep

\subsubsection{Applications} % 1 page

% Incrementation (avoid cache effects in binarization)

% average (1 shuffle)
% Nipype: can't use SLURM plugin so had to actively poll.

% kmeans (many shuffles)

% Example BIDS app

% fmriprep?

% Explain how we tweaked scheduling in Nipype to make it similar to Spark, 
% otherwise we're only measuring scheduling differences. Needs a discussion on scheduling somewhere.

% Talk about our I/O pattern: random I/O, sequential.

\begin{table}
\centering
\begin{tabular}{c|c}
Application & $\frac{D}{C}$ (GB/s)\\
\hline
Big Brain            & [0-75] \\
fMRI pre-processing  & [0-1.5]
\end{tabular}
\label{table:applications}
\caption{Data-compute ratios for typical neuroinformatics applications}
\end{table}
% fMRI: about 100M in, 2GB out,2 30 min processing time to 10 hours



\subsubsection{Execution modes} % perhaps find a better title, it's not informative

We tuned job scheduling to balance the load among
cluster nodes. This is the default behavior of Spark, but required
specific instrumentation in Nipype.

% separate this in (1) the concepts, put in the model sections below, 
% (2) the implementation 'details', put in the experiments

% in-memory (Spark only)

% tmpfs: two extra in-memory copies. memfs could address that but we don't have it yet.
% Spark: when just passing filenames. turns out i need to set `preservePartitioning` to True
% local disk: write buffer issue

% shared fs: Lustre


\section{Results} % 2 pages
\label{sec:results}

% We checked that scheduling was similar in both cases (show a two Gantt charts to illustrate)

\subsection{Model Evaluations}

\subsection{CPU Time}
% Comment on the general trend
% We can't really say that imaging tasks are not small. Even Freesurfer could be decomposed.
% 

\subsection{Image Block size}
% Comment on the general trend

\subsection{Data Size}
% Comment on the general trend

\subsection{Iterations}
% Comment on the general trend

% Interactive gantt charts?

\section{Discussion} % 2 pages
\label{sec:discussion}
\subsection{Effect of In-Memory Computing}
% Shared fs vs local disk vs in-memory: what do we gain?

\subsection{Effect of Data Locality}
% It should be important, in particular when there is contention and when
% tasks are small. 


% Effect of Lazy Evaluation?


\subsection{Can \texttt{tmpfs} and Page Caches Emulate In-Memory Computing?}
% tmpfs and local disks fill up: need cleanup.
% Can we emulate in-memory computing using write buffers or tmpfs?


\subsection{Scheduling Remarks}
% Scheduling: load balance as much as you can.
% (which isn't what nipype multiproc does)

\subsection{Other Comments}

% fault-tolerance: do we have anything to discuss here?

\subsection{Engine-Specific Comparisons}

% Spark vs Nipype
% Do we see the effect of Java serialization?

\section{Conclusion} % 1 page with refs
\label{sec:conclusion}
% When is it useful to use a Big Data engine?

% "```Disk and network I/O, of course, play a part in Spark performance 
% as well, but neither Spark nor YARN currently do anything to actively 
% manage them.```"
% http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/


% Future work:
% - scheduling in a shared environment.
% - workflow-aware cache eviction strategies instead of LRU/n

\section{Acknowledgments}

\todo{Acknowledge Dell and ask them if they want to co-author the paper.}

\bibliographystyle{IEEEtran} 
\bibliography{biblio}

\end{document}























